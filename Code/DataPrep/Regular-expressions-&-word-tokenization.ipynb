{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, you'll learn natural language processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier.\n",
    "\n",
    "You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning.\n",
    "\n",
    "This Notebook will introduce some basic NLP concepts, such as word tokenization and regular expressions to help parse text. You'll also learn how to handle non-English text and more difficult tokenization you might find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = pd.read_table(\"https://assets.datacamp.com/production/repositories/932/datasets/8042ed46ae7faef4951fcda771c5acc4fc3c0bf6/english_stopwords.txt\")\n",
    "grail = pd.read_table(\"https://assets.datacamp.com/production/repositories/932/datasets/4921d0bf6a73fd645f49f528faf74a871bb3a0e9/grail.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with package re (regular expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>myself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>shouldn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>wasn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>weren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>wouldn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           i\n",
       "0         me\n",
       "1         my\n",
       "2     myself\n",
       "3         we\n",
       "4        our\n",
       "..       ...\n",
       "147  shouldn\n",
       "148     wasn\n",
       "149    weren\n",
       "150      won\n",
       "151   wouldn\n",
       "\n",
       "[152 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['being', 'having', 'doing', 'during']\n"
     ]
    }
   ],
   "source": [
    "mylist = list(english_stopwords.iloc[:,0])\n",
    "\n",
    "r = re.compile(\".*ing\")\n",
    "newlist = list(filter(r.match, mylist)) # Read Note below\n",
    "\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'are', 'a', 'an', 'and', 'as', 'at', 'about', 'against', 'after', 'above', 'again', 'all', 'any', 'ain', 'aren']\n"
     ]
    }
   ],
   "source": [
    "r = re.compile(\"^a\")\n",
    "newlist = list(filter(r.match, mylist)) # Read Note below\n",
    "\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to tokenization.\n",
    "\n",
    "* Turning a string or document into tokens (smaller chunks)\n",
    "\n",
    "* One step in preparing a text for NLP\n",
    "\n",
    "* Many different theories and rules\n",
    "\n",
    "* You can create your own rules using regular expressions\n",
    "\n",
    "* Some examples : \n",
    "\n",
    "    Breaking out workds or sentences\n",
    "\n",
    "    Separating punctuation\n",
    "    \n",
    "    Separating all hashtag in a tweet\n",
    "\n",
    "NLTK Library : Natural language toolkit\n",
    "\n",
    "Why tokenize ?\n",
    "\n",
    "* Easier to map part of speech \n",
    "\n",
    "* Matching common words \n",
    "\n",
    "* Removing unwanted tokens\n",
    "\n",
    "* \"I don't like Sam's shoes. -> \"\"I\",\"do\",\"n't\",\"like\",\"Sam\",\"'s\",\"shoes\",\".\"\n",
    "\n",
    "Other nltk tokenizers\n",
    " \n",
    "* sent_tokenize : tokenize a document into sentences \n",
    "* regexp_tokenize : tokenize a string or document based on a regular expression paern \n",
    "* TweetTokenizer : special class just for tweet tokenization, allowing you to separate hashtags, mentions and lot sof exclamation points!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List with elements text :  SOLDIER #1: Pull the other one!\n",
      "Tokenization one element of text :  ['SOLDIER', '#', '1', ':', 'Pull', 'the', 'other', 'one', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "#Split scene_one into sentences: sentences\n",
    "#nltk.download('punkt')\n",
    "\n",
    "grail_list = list(grail.iloc[:,0])\n",
    "print(\"List with elements text : \" , grail_list[3])\n",
    "\n",
    "print(\"Tokenization one element of text : \" , word_tokenize(grail_list[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization of all the elements of list, unique words in list with text :  {'I', '1', 'am', '1187', 'Christ', 'and', \"'s\", 'Halt', '1189', 'serva', 'Uther', '2', 'Pendrago', '1186', ']', 'that', '0', 'SOLDIER', ',', 'object', 'one', '3', 'Pull', 'son', '1188', 'KING', '[', 'sonny', 'That', 'wind', 'Arthur', 'Length', '.', 'goes', 'the', 'Who', '...', 'My', 'INSPECTOR', 'squeak', 'of', '1185', 'Everything', 'OFFICER', 'offe', 'an', 'there', 'with', '?', '!', 'on', '#', 'off', 'right', '1190', 'other', \"'em\", 'clop', 'trusty', 'dtype', 'R', 'Whoa', 'It', 'is', 'Come', 'CAMERAMAN', 'Name', 'Back', 'ARTHUR', 'this', 'All', '4', 'SCENE', 'my', ':', 'enough'}\n",
      "------\n",
      "Count unique words :  76\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(word_tokenize(str(grail.iloc[:,0])))\n",
    "\n",
    "print(\"tokenization of all the elements of list, unique words in list with text : \", unique_words)\n",
    "print(\"------\")\n",
    "print(\"Count unique words : \", len(unique_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More regex with re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find verbs with contains ing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b72cda31977e5af0c9a2ec7bfa1f749a44f23179b1e93c96f5dbff5d5c61f18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
